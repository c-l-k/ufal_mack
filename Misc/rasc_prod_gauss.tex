\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}                   % AAB inserido
\usepackage[utf8]{inputenc}             % AAB inserido
\usepackage{rotating}                   % AAB inserido
%\usepackage[round,sort,nonamebreak]{natbib} % citação bibliográfica textual
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
%AAB
\DeclareMathOperator{\traco}{tr}
\begin{document}

\title{Paper Title*\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Bolsista Capes.}
}

\author{\IEEEauthorblockN{Anderson A. de Borba}
\IEEEauthorblockA{\textit{Dept. Engenharia Elétrica e Computação} \\
\textit{UPM - Universidade Presbiteriana Mackenzie}\\
IBMEC-SP\\
São Paulo, Brazil \\
anderson.borba@ibmec.edu.br}
\and
\IEEEauthorblockN{Maurício Marengoni}
\IEEEauthorblockA{\textit{Dept. Engenharia Elétrica e Computação} \\
\textit{UPM - Universidade Presbiteriana Mackenzie}\\
São Paulo, Brazil \\
mauricio.marengoni@mackenzie.br}
\and
\IEEEauthorblockN{\hspace{6cm} Alejandro C. Frery}
\IEEEauthorblockA{\textit{\hspace{6cm}Laboratório de Computação Científica e Análise Numérica} \\
\hspace{6cm}\textit{UFAL - Universidade Federal de Alagoas)}\\
\hspace{6cm} Maceió, Brazil \\
\hspace{6cm}acfrery@gmail.com}
}

\maketitle

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
\section{Modelagem estatística para dados PolSAR}
Os sistemas SAR totalmente polarimétricos transmitem pulsos de micro-ondas polarizados ortogonalmente e medem componentes ortogonais do sinal recebido. Para cada pixel, a medida resulta em uma matriz de coeficientes de espalhamento. Esses coeficientes são números complexos que descrevem no sistema SAR a transformação do campo eletromagnético transmitido para o campo eletromagnético recebido.

A transformação pode ser representada como
\begin{equation}
 \left[
\begin{array}{c}
	E_{h}^{r}   \\
	E_{v}^{r}    \\
\end{array}
\right]
 = \frac{e^{\hat{\imath} kr}}{r}\left[
\begin{array}{cc}
	S_{hh}   & S_{hv}   \\
	S_{vh}   & S_{vv}   \\
\end{array}
\right]
 \left[
\begin{array}{c}
	E_{h}^{t}   \\
	E_{v}^{t}    \\
\end{array}
\right],
\end{equation}
onde $k$ denota o número de onda, $\hat{\imath}$ é um número complexo e $r$ é a distância entre o radar e o alvo. O campo eletromagnético com componentes $E_{i}^{j}$, o índice subscrito denota polarização horizontal ($h$) ou vertical ($v$),  enquanto o índice sobrescrito indica a onda recebida ($r$) ou transmitida ($t$). Definindo $S_{i,j}$ como os coeficientes de espalhamento complexo, tal que o índice $i$ e $j$ são associados com o recebimento com a transmissão das ondas, por exemplo, o coeficiente de espalhamento $S_{hv}$ está associado a onda transmitida na direção vertical ($v$) e recebida na direção horizontal ($h$).

Sendo conhecido cada um dos coeficientes, a matriz de espalhamento complexa $\mathbf{S}$ é definida por
\begin{equation}
\mathbf{S} = \left[
\begin{array}{cc}
	S_{hh}   & S_{hv}   \\
	S_{vh}   & S_{vv}   \\
\end{array}
\right],
\end{equation}
se o meio de propagação das ondas é recíproco, isto é, de uma maneira geral as propriedades de transmissão e recebimento de uma antena são idênticos, então usaremos o teorema da reciprocidade \cite{lp} para definir a matriz de espalhamento como sendo hermitiana, ou seja, a igualdade dos termos complexos $S_{hv}=S_{vh}$. De acordo com o teorema da reciprocidade a matriz de espalhamento pode ser representada pelo vetor
\begin{equation}
\mathbf{S} = \left[
\begin{array}{c}
	S_{hh}     \\
    S_{vh}     \\
	S_{vv}     \\
\end{array}
\right].
\end{equation}

Sabemos que cada componente do vetor $\mathbf{S}$ é complexo, com intuito de fixar notação reescrevemos,
\begin{equation}
\begin{array}{ccc}
	S_{hh} && R_{hh} + i I_{hh}    \\
    S_{vh} &=& R_{hv} + i I_{hv}   \\
	S_{vv} &=& R_{vv} + i I_{vv}   \\
\end{array}
\end{equation}
Poderíamos considerar um vetor de dimensão $6$ onde cada entrada está distribuída como $N(0,\sigma)$.
\begin{equation}
\mathbf{S} = \left[
\begin{array}{c}
	R_{hh}     \\
    I_{hh}     \\
	R_{hv}     \\
	I_{hh}     \\
    R_{vv}     \\
	I_{vv}     \\
\end{array}
\right].
\end{equation}

Por hipótese teremso a distribuição gaussiana circular complexa multivariada com média zero pode ser definida de acordo com \cite{good}, assim, sendo $\mathbf{S}_{ij}= R_{ij}+ i I_{ij}$ é exigido que $R_{ij}$ e $y_{ij}$ com $j=h,v$ tenham distribuições conjuntas gaussianas e satisfaçam as seguintes condições 
\begin{itemize}
    \item[-] obs: entender e reescrever melhor essas hipóteses,
	\item[-] $E[R_{ij}]=E[I_{ij}]=0$,
	\item[-] $E[R_j^2]=E[I_j^2]$,
	\item[-] $E[R_jI_j]=0$,
	\item[-] $E[R_jR_i]=E[I_jI_i]$,
	\item[-] $E[I_jR_i]=-E[R_jR_i]$,
\end{itemize}
onde, $E[\cdot]$ é o valor esperado.


De acordo com \cite{good} e \cite{lee} esta distribuição pode modelar adequadamente o comportamento estatístico de $\mathbf{S}$. A hipotêse de ser gaussiana e circular foi comprovada para dados SAR polarimétricos no artigo \cite{sarabendi}.   


A matriz de covariância associada a $\mathbf{S}$ definida por
\begin{equation}
	{\bf C_{{\bf S}}} = E[{\mathbf S}{\mathbf S}^H] = \left[
\begin{array}{cc}
	E[S_{hh}\overline{S_{hh}}]  & E[{S_{hh}}\overline{S_{hv}}]   \\
	E[S_{hv}\overline{S_{hh}}]  & E[{S_{vv}}\overline{S_{vv}}]  \\
\end{array}
\right]
\end{equation}
talque, $\overline{\cdot}$ denota o conjugado complexo. A matriz de covariância é hermitiana positiva definida e contém todas as informações necessárias para caracterizar o retroespalhamento, podemos consultar mais informações em \cite{mfp}. 

Nas imagens PolSAR serão consideradas três componentes para o vetor $\mathbf{S}=[S_{hh},S_{vh},S_{vv}]^T$ e a multiplicação de $\mathbf{s}=[S_{hh},S_{vh},S_{vv}]$ pelo seu conjugado transposto $\mathbf{S}=[S_{hh},S_{vh},S_{vv}]^H$, isto é, a hermitiana do vetor, 

\begin{equation}
\mathbf{s}\mathbf{s}^H = \left[
\begin{array}{c}
	S_{hh}      \\
        S_{vh}     \\
	S_{vv}      \\
\end{array}
\right]
\left[
\begin{array}{ccc}
	S_{hh}  & S_{vh}  & S_{vv}      \\
\end{array}
\right]^H = \left[S_{ij} \right]_{i,j=h,v}
\end{equation}

De acordo com \cite{good} a distribuição gaussiana complexa multivariada pode modelar adequadamente o comportamento estatístico de $\boldmath S$. Isto é chamado de {\it single-look complex PolSAR data representation} e podemos definir o vetor de espalhamento por $\mathbf{S}=[S_{hh},S_{hv},S_{vv}]^H$. 

Dados polarimétricos são usualmente sujeitados a um processo de várias visadas com o intuito de melhorar a razão entre o sinal e o seu ruído. Para esse fim, matrizes positivas definidas hermitianas estimadas são obtidas computando a média de $L$ visadas independentes de uma mesma cena. Resultando na matriz de covariância amostral estimada {\bf Z} conforme \cite{good, ade}
\begin{equation}
\begin{array}{ccc}
    \mathbf{Z}&=&\frac{1}{L}\displaystyle{\sum_{l=1}^{L} {\mathbf{s}_l}{\mathbf{s}_l}^H}, \\
\end{array}
\end{equation}
onde $\mathbf{s}_l$ com $l = 1, \dots, L$ é uma amostra de $\mathit{L}$ vetores complexos distribuídos como $\mathbf{S}$, assim a matriz de covariância amostral associada a $\mathbf{S}_l$, com $l=1,\dots,L$ denotam o espalhamento para cada visada $L$

Sendo $i=j$
\begin{equation}
\begin{array}{ccc}
\mathbf{S}_{ii}\overline{\mathbf{S}}_{ii}&=& (R_{ii}+iI_{ii})\overline{(R_{ii}+iI_{jj})} \\
\mathbf{S}_{ii}\overline{\mathbf{S}}_{ii}&=& (R_{ii}+iI_{ii})(R_{ii}-iI_{ii}) \\
\mathbf{S}_{ii}\overline{\mathbf{S}}_{ii}&=& R_{ii}^2+I_{ii}^2 \\
\end{array}
\end{equation}
e considerando $i \neq j$
\begin{equation}
\begin{array}{ccc}
\mathbf{S}_{ii}\overline{\mathbf{S}}_{ij}&=& (R_{ii}+iI_{ii})\overline{(R_{ij}+iI_{ij})} \\
\mathbf{S}_{ii}\overline{\mathbf{S}}_{ij}&=& (R_{ii}+iy_{ii})(I_{ij}-iI_{ij}) \\
\mathbf{S}_{ii}\overline{\mathbf{S}}_{ij}&=& (R_{ii}R_{ij}+I_{ii}I_{ij})+i(R_{ij}I_{ii}-R_{ii}I_{ij}) \\
\end{array}
\end{equation}
 Definindo,
 \begin{equation}
\begin{array}{ccc}
	  RC_{ij}&=&  R_{ii}R_{ij}+I_{ii}I_{ij} 
\end{array}
\end{equation}
e
\begin{equation}
\begin{array}{ccc}
	  IC_{ij}&=& R_{ij}I_{ii}-R_{ij}I_{ii}
\end{array}
\end{equation}

Sendo a variável randômica gaussiana complexa $\mathbf{C_{i,j}}=RC_{ij} + i IC_{ij}$, ou ainda, $\mathbf{C_{i,j}}=(R_{ii}R_{ij}+I_{ii}I_{ij}) + i(R_{ij}I_{ii}-R_{ij}I_{ii})$. Podemos escrever uma variável randômica gaussiana complexa $4-$variada $(R_{ii},R_{ij},I{ii},I_{ij})$.

De acordo com (\cite{good})
\begin{equation}
\mathbf{C} = \left[
\begin{array}{cc}
	E(X_iX_j)  & E(X_iY_j)  \\
	E(Y_iX_j)  & E(Y_iY_j)  \\
\end{array}
\right].
\end{equation}
Tal que
\begin{equation}
\mathbf{C} =
\left\{
\begin{array}{cc}
	\frac{1}{2}\left[
\begin{array}{cc}
	 1 & 0  \\
	 0 & 1  \\
\end{array}
	\right]\sigma^{2}_{j}  & \mbox{se}\quad i=j, \\
	& \\
	\frac{1}{2}\left[
\begin{array}{cc}
	\alpha_{ij} & -\beta_{ij}  \\
	 \beta_{ij} & \alpha_{ij}  \\
\end{array}
	\right]\sigma_j\sigma_k  & \mbox{se}\quad i\neq j.   \\
\end{array}
\right.
\end{equation}

\begin{sidewaystable}
	\centering
	\caption{Tabela}
\begin{tabular}{@{}lcccccc@{}} \toprule
	     &$R_{hh}$        & $I_{hh}$ & $R_{hv}$&$I_{hv}$                            &$R_{vv}$                           &$I_{vv}$ \\ \midrule
$R_{hh}$ &$\sigma_{hh}^2$ & 0                  &$\rho_{hh,hv}\sigma_{hh}\sigma_{hv}$ &$\eta_{hh,hv}\sigma_{hh}\sigma_{hv}$ & $\rho_{hh,vv}\sigma_{hh}\sigma_{vv}$&$\eta_{hh,vv}\sigma_{hh}\sigma_{vv}$  \\ 
	$I_{hh}$ & 0 & $\sigma_{hh}^2$ &$-\eta_{hh,hv}\sigma_{hh}\sigma_{hv}$ &$\rho_{hh,hv}\sigma_{hh}\sigma_{hv}$ &$-\eta_{hh,vv}\sigma_{hh}\sigma_{vv}$ &$\rho_{hh,vv}\sigma_{hh}\sigma_{vv}$  \\ 
	$R_{hv}$ &$\rho_{hh,hv}\sigma_{hh}\sigma_{hv}$   &$-\eta_{hh,hv}\sigma_{hh}\sigma_{hv}$  &$\sigma_{hv}^2$ &0 &$\rho_{hv,vv}\sigma_{hv}\sigma_{vv}$ &$\eta_{hv,vv}\sigma_{hv}\sigma_{vv}$  \\ 
	$I_{hv}$ &$\eta_{hh,hv}\sigma_{hh}\sigma_{hv}$  &$\rho_{hh,hv}\sigma_{hh}\sigma_{hv}$  &0 &$\sigma_{hv}^2$ &$-\eta_{hv,vv}\sigma_{hv}\sigma_{vv}$ &$\rho_{hv,vv}\sigma_{hv}\sigma_{vv}$ \\ 
	$R_{vv}$ &$\rho_{hh,vv}\sigma_{hh}\sigma_{vv}$  &$-\eta_{hh,vv}\sigma_{hh}\sigma_{vv}$  &$\rho_{hv,vv}\sigma_{hv}\sigma_{vv}$ &$-\eta_{hv,vv}\sigma_{hv}\sigma_{vv}$ & $\sigma_{vv}^2$ &0 \\ 
    $I_{vv}$ &$\eta_{hh,vv}\sigma_{hh}\sigma_{hv}$  &$\rho_{hh,vv}\sigma_{hh}\sigma_{vv}$  &$\eta_{hv,vv}\sigma_{hv}\sigma_{vv}$ &$\rho_{hv,vv}\sigma_{hv}\sigma_{vv}$ & 0 &$\sigma_{vv}^2$ \\ 	 \bottomrule
\end{tabular}
\end{sidewaystable}
\section{Funções de densidade}
Sendo $(R_{ii}, R_{ij})\sim N2(0, C_{ij})$ podemos observar na tabela anterior que 
\begin{equation}
C_{ij}=\left[
\begin{array}{cc}
	\sigma_{ii}^2   &  \rho_{ii,ij}\sigma_{ii}\sigma_{ij}  \\
	\rho_{ii,ij}\sigma_{ii}\sigma_{ij} & \sigma_{ij}^2   \\
\end{array}
\right],
\end{equation}
A pdf para esta distribuição normal é:

\begin{equation}
\begin{array}{ccc}
	f_{Z_{R_{ii}R_{ij}}}(z)&=&\frac{1}{\pi\sigma_{ii}\sigma_{ij}\sqrt{1-\rho_{ii,ij}^2}}\exp\left(\frac{\rho_{ii,ij}z}{\sigma_{ii}\sigma_{ij}(1-\rho_{ii,ij})^2}\right)\\
	&&K_0\left(\frac{|z|}{\sigma_{ii}\sigma_{ij}(1-\rho_{ii,ij})^2}\right).
\end{array}
\end{equation}

Definindo o funcional $\Theta(z;\sigma_p,\sigma_q,\gamma)$
\begin{equation}
\begin{array}{ccc}
	\Theta(z;\sigma_p,\sigma_q,\gamma)&=&\frac{1}{\pi\sigma_p\sigma_q\sqrt{1-\gamma^2}}\exp\left(\frac{\gamma z}{\sigma_p\sigma_q(1-\gamma)^2}\right)\\
	&&K_0\left(\frac{|z|}{\sigma_p\sigma_q(1-\gamma)^2}\right).
\end{array}
\end{equation}
Sendo $(I_{ii}, I_{ij})\sim N2(0, C_{ij})$ podemos observar na tabela anterior que 
\begin{equation}
C_{ij}=\left[
\begin{array}{cc}
	\sigma_{ii}^2   &  \rho_{ii,ij}\sigma_{ii}\sigma_{ij}  \\
	\rho_{ii,ij}\sigma_{ii}\sigma_{ij} & \sigma_{ij}^2   \\
\end{array}
\right],
\end{equation}
obs: mesma distribuição!!!!!!
\begin{equation}
\begin{array}{ccc}
	f_{Z_{R_{ii}R_{ij}}}(z)&=&\frac{1}{\pi\sigma_{ii}\sigma_{ij}\sqrt{1-\rho_{ii,ij}^2}}\exp\left(\frac{\rho_{ii,ij}z}{\sigma_{ii}\sigma_{ij}(1-\rho_{ii,ij})^2}\right)\\
	&&K_0\left(\frac{|z|}{\sigma_{ii}\sigma_{ij}(1-\rho_{ii,ij})^2}\right).
\end{array}
\end{equation}

\section{distribuicão conjunta}
Sendo $(R_{ii}, R_{ij},I_{ii}, I_{ij})\sim N4(0, C_{ii,ij})$ podemos observar na tabela anterior que 
\begin{equation}
C_{ii,ij}=\left[
\begin{array}{cccc}
	\sigma_{ii}^2   &  \rho_{ii,ij}\sigma_{ii}\sigma_{ij} & 0&\eta_{ii,ij}\sigma_{ii}\sigma_{ij}\\
	\rho_{ii,ij}\sigma_{ii}\sigma_{ij} & \sigma_{ij}^2  & -\eta_{ii,ij}\sigma_{ii}\sigma_{ij}&0 \\
	0&-\eta_{ii,ij}\sigma_{ii}\sigma_{ij}&\sigma_{ii}^2&\rho_{ii,ij}\sigma_{ii}\sigma_{ij}\\
	\eta_{ii,ij}\sigma_{ii}\sigma_{ij}&0&\rho_{ii,ij}\sigma_{ii}\sigma_{ij}&\sigma_{ij}^2\\
\end{array}
\right],
\end{equation}
Realizar a transformação 
\begin{equation}
\left[
\begin{array}{ccc}
	 Z = R_{ii}R_{ij}+I_{ii}I_{ij} \\
	 U_1 = R_{ii}\\
	 U_2 = R_{ij}\\
	 U_3 = I_{ii}\\
\end{array}
\right],
\end{equation}
\section{Deteção de borda}
\subsection{Detecção de bordas nas bandas $I_{hh}$, $I_{hv}$ e $I_{vv}$}
Dados polarimétricos são usualmente sujeitados a um processo de várias visadas com o intuito de melhorar a razão entre o sinal e o seu ruído. Para esse fim, matrizes positivas definidas hermitianas estimadas são obtidas computando a média de $L$ visadas independentes de uma mesma cena. Resultando na matriz de covariância amostral estimada {\bf Z} conforme \cite{good, ade}
\begin{equation}
\begin{array}{ccc}
    \mathbf{Z}&=&\frac{1}{L}\displaystyle{\sum_{i=1}^{L} {\mathbf{s}_i}{\mathbf{s}_i}^H}, \\
\end{array}
\end{equation}
onde $\mathbf{s}_i$ com $i = 1, \dots, L$ é uma amostra de $\mathit{L}$ vetores complexos distribuídos como $\mathbf{s}$, assim a matriz de covariância amostral associada a $\mathbf{s}_i$, com $i=1,\dots,L$ denotam o espalhamento para cada visada $L$ seguindo uma distribuição complexas de Wishart. 

Sendo agora $\mathbf{\Sigma_{s}}$ e $L$ parâmetros conhecidos a função densidade de probabilidade da distribuição Wishart por  
%
\begin{equation}
    f_{\mathbf{Z}}(\mathbf{Z};\mathbf{\Sigma_{s}},L)=\frac{L^{mL}|\mathbf{Z}|^{L-m}}{|\mathbf{\Sigma_{s}}|^{L}\Gamma_m(L)} \exp(-L\traco(\mathbf{\Sigma_{s}}^{-1}\mathbf{Z})), \\
\end{equation}
onde, $\traco(\cdot)$ é o operador traço de uma matriz, $\Gamma_m(L)$ é uma função Gamma multivariada definida por
\begin{equation}
	\Gamma_m(L)=\pi^{\frac{1}{2}m(m-1)} \prod_{i=0}^{m-1}\Gamma(L-i) \\
\end{equation}
e $\Gamma(\cdot)$ é a função Gamma.

\subsection{Método da máxima verosimilhança}

O conceito de verossimilhança é importante em estatística, pois descreve de maneira precisa as informações sobre os parâmetros do modelo estatístico que representa os dados observados. De maneira geral, a estimativa por máxima verossimilhança (\textbf{MLE}) é um método que, tendo um conjunto de dados e um modelo estatístico, estima os valores dos parâmetros do modelo estatísticos com intuito de maximizar uma função de probabilidade dos dados. 

Suponha $\mathbf{X}=(X_1,X_2,\dots,X_n)^T$ um vetor randômico distribuído de acordo com a $\mathbf{p.d.f}$ função densidade de probabilidade $f(\mathbf{x},\mathbf{\theta})$ com parâmetros $\mathbf{\theta}=(\theta_1,\dots,\theta_d)^T$ no espaço dos parâmetros $\Theta$. Definimos  a \textbf{função de verossimilhança} cuja amostra é 
\begin{equation}
    L(\theta;\mathbf{X}) = \prod_{i=1}^{n}f(x_i;\theta), \\
\end{equation}
e a função logarítmica de verossimilhança a qual podemos chamar de  \textbf{função de log-verossimilhança} é a soma
\begin{equation}
	l(\theta;\mathbf{X})= \ln(L(\theta;\mathbf{X})) = \sum_{i=1}^{n}\ln(f(x_i;\theta)). \\
\end{equation}

Podemos definir o método da \textbf{estimativa de máxima verossimilhança} (\textbf{MLE}) de $\theta$ como sendo o vetor $\widehat{\theta}$ tal que $L(\widehat{\theta};\mathbf{x})\ge L(\theta;\mathbf{x})$ para todo $\theta$ no espaço dos parâmetros $\Theta$. De maneira simplificada a \textbf{estimativa de máxima verossimilhança} pode ser escrita por 
\begin{equation}
    \widehat{\theta}= \text{arg}\,\max\limits_{\theta\in\Theta}L(\theta;\mathbf{x}),\\
\end{equation}
ou de maneira similar 
\begin{equation}
    \widehat{\theta}= \text{arg}\,\max\limits_{\theta\in\Theta}l(\theta;\mathbf{x}).\\
\end{equation}

\subsection{Estimativa de máxima verossimilhança aplicada a distribuíção Wishart}

Nesta seção vamos usar o método de máxima verossimilhança aplicado na distribuição Wishart. Suponha $\mathbf{Z}=(\mathbf{Z}_1,\mathbf{Z}_2,\dots,\mathbf{Z}_N)^T$ um vetor randômico distribuído de acordo com a $\mathbf{p.d.f}$ função densidade de probabilidade (\ref{cap_acf_9}) com parâmetros $\Sigma=\{\mathbf{\Sigma_A}, \mathbf{\Sigma_B\}}$ e $L$.

A \textbf{função de verossimilhança} da amostra descrita por (\ref{cap_acf_13}) é dada pela equação produtório das funções de densidade respectivamente associadas a cada amostra
\begin{equation}
	L(j)=\prod_{k=1}^{j}f_{\mathbf{Z}}(\mathbf{Z}_{k}^{'};\mathbf{\Sigma_{A}},L) \prod_{k=j+1}^{N}f_{\mathbf{Z}}(\mathbf{Z}_{k}^{'};\mathbf{\Sigma_{B}},L), \\
\end{equation}
onde $\mathbf{Z}_{k}^{'}$ é uma possível aproximação da matriz randômica descrita em (\ref{cap_acf_13}).

Usando a equação (\ref{cap_acf_12}) e propriedades de logaritmos natural teremos para cada termo do produtório (\ref{cap_acf_18}) uma \textbf{função de log-verossimilhança}. Assim, encontramos uma função dependente de $j$
\begin{equation}
\begin{array}{rcl}
	l(j)=\ln L(j)&=&\sum_{k=1}^{j}\ln f_{\mathbf{Z}}(\mathbf{Z}_{k}^{'};\mathbf{\Sigma_{A}},L)\\
	             &+&\sum_{k=j+1}^{N}\ln f_{\mathbf{Z}}(\mathbf{Z}_{k}^{'};\mathbf{\Sigma_{B}},L).
\end{array}
\end{equation}

Nesse momento, podemos realizar  manipulações algébricas na função densidade de probabilidade em cada termo do somatório conforme (\ref{cap_acf_12}) e substituir nas duas parcelas da equação (\ref{cap_acf_19}) levando em consideração que as duas amostras são diferentes, desta forma
\begin{equation}
\begin{array}{lll}
	l(j)&=&N\left[mL\ln{\left(L\right)}-\ln{\left(\Gamma_m(L)\right)}\right]\\
	&-& L\left[j\ln{\left(|\mathbf{\Sigma_{A}}|\right)}+(N-j)\ln{\left(|\mathbf{\Sigma_{B}}|\right)}\right] \\
	&+&(L-m)\sum_{k=1}^{N}\ln{\left(|\mathbf{Z}_{k}^{'}|\right)}\\
	&-&L\left[\sum_{k=1}^{j}tr(\mathbf{\Sigma_{A}}^{-1}\mathbf{Z}_{k}^{'})+ \sum_{k=j+1}^{N}tr(\mathbf{\Sigma_{B}}^{-1}\mathbf{Z}_{k}^{'})\right]. \\
\end{array}
\end{equation}

A matriz $\Sigma$ pode ser encontrada usando o estimador de máxima verossimilhança denotado por $\widehat{\Sigma}$ de acordo com a referência \cite{good}. A equação (\ref{cap_acf_20}) representa duas estimativa para a matriz de covariância $\Sigma$ que dependem da posição $j$
\begin{equation}
\widehat{\Sigma_{I}}(j) = \left\{
\begin{array}{lc}
	j^{-1}\sum_{k=1}^{j}\mathbf{Z}_{k}  & \mbox{se}\quad I=A,  \\
        (N-j)^{-1}\sum_{k=j+1}^{N}\mathbf{Z}_{k} & \mbox{se}\quad I=B. \\
\end{array}
\right.
\end{equation}

Usando a equação (\ref{cap_acf_20}) podemos substituir na equação acima e continuar a manipulação algébrica
portanto, 
\begin{equation}
\begin{array}{rcl}
	l(j)&=&N\left[-mL(1-\ln{\left(L\right)})-\ln{\left(\Gamma_m(L)\right)}\right]\\
	&-&L\left[j\ln{\left(|\mathbf{\widehat{\Sigma}}_{A}(j)|\right)} +(N-j)\ln{\left(|\mathbf{\widehat{\Sigma}}_{B}(j)|\right)}\right], \\
	&+&(L-m)\sum_{k=1}^{N}\ln{\left(|\mathbf{Z}_{k}^{'}|\right)}. \\
\end{array}
\end{equation}

O estimador de máxima verossimilhança $\widehat{\jmath}_{ML}$ é uma evidência de borda por representar uma aproximação da transição de região e pode ser calculado pelo método de maximização
\begin{equation}
\begin{array}{rcl}
	\widehat{\jmath}_{ML}&=&\text{arg}\max\limits_{j}l(j).  \\
\end{array}
\end{equation}
\section{Métodos de fusão}
\subsection{Média simples}
Ref (\cite{mit})
\subsection{PCA}
Ref (\cite{n_r})
\subsection{SWT}
Ref (\cite{n_r})
\subsection{Estatística ROC}
\cite{gs} e \cite{fawcett}
\section{Métricas}
\subsection{RMSE - Root mean square}
\begin{equation}
	RMSE=\sqrt{\frac{1}{MN}\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j)-I_f(i,j))^2}.  \\
\end{equation}
\subsection{MAE - Root Mean absolute}
\begin{equation}
	RMSE=\frac{1}{MN}\sum_{i=1}^M\sum_{j=1}^N\left|I_r(i,j)-I_f(i,j)\right|.  \\
\end{equation}
\subsection{Percent fit error}
\begin{equation}
	PFE=\frac{norm(I_r-I_f)}{norm(I_r)}*100.  \\
\end{equation}
\subsection{Signal to noise ratio}
\begin{equation}
SRN = 20log_{10}\left(\frac{\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j))^2}{\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j)-I_f(i,j))^2}\right)
\end{equation}
\subsection{Peak signal to noise ratio}
\begin{equation}
PSRN = 20log_{10}\left(\frac{L^2}{\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j)-I_f(i,j))^2}\right)
\end{equation}
Aqui $L$ é o número de níveis de cinza na imagem. 
\subsection{Correlaçao}
\begin{equation}
CORR = \frac{2C_{rf}}{C_r+Cf}
\end{equation}
Onde $$C_r= \sum_{i=1}^M\sum_{j=1}^N(I_r(i,j))^2,$$ $$C_f=\sum_{i=1}^M\sum_{j=1}^N(I_f(i,j))^2,$$ $$C_{rf}=\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j)I_f(i,j)),$$
Aqui $L$ é o número de níveis de cinza na imagem. 

\subsection{Mutual information}
\begin{equation}
MI = \sum_{i=1}^M\sum_{j=1}^N h_{I_rI_f}(i,j){log_2\left(\frac{h_{I_rI_f}(i,j)}{h_{I_r}(i,j)h_{I_f}(i,j)}\right)}
\end{equation}
\subsection{Universal quality index}
\begin{equation}
QI=\frac{4\sigma_{I_rI_f}(\nu_{I_r}+\nu_{I_f})}{(\sigma_{I_r}^2+\sigma_{I_f}^2)(\nu_{I_r}^2+\nu_{I_f}^2)}
\end{equation}
onde 
$$\nu_{I_r}=\frac{1}{MN}\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j))^2,$$
$$\nu_{I_f}=\frac{1}{MN}\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j))^2,$$ $$\sigma_{I_r}^2=\frac{1}{MN-1}\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j)-\mu_{I_r})^2,$$
$$\sigma_{I_f}^2   =\frac{1}{MN-1}\sum_{i=1}^M\sum_{j=1}^N(I_f(i,j)-\mu_{I_f})^2$$ e
$$\sigma_{I_rI_f}^2=\frac{1}{MN-1}\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j)-\mu_{I_r})(I_f(i,j)-\mu_{I_f})$$ 
\subsection{Measure of structural similarity}
\begin{equation}
SSIM=\frac{(2\nu_{I_r}\nu_{I_r}+C_1)(2\sigma_{I_rI_f}+C_2)}{(\mu_{I_r}^2+\nu_{I_f}^2+C_1)(\sigma_{I_r}^2+\sigma_{I_f}^2+C_2)}
\end{equation}
onde $C_1$ é uma constante que é incluída para evitar a instabilidade quando $(\mu_{I_r}^2+\nu_{I_f}^2+C_1)$ e $C_2$ é uma constante que é incluída para evitar a instabilidade quando $(\sigma_{I_r}^2+\sigma_{I_f}^2+C_2)$ é perto de zero.
\subsection{Standard deviation}
\begin{equation}
\sigma=\sqrt{\sum_{i=1}^L(i-\bar{i})^2h_{I_f}(i)}
\end{equation}
onde $\bar{i}=\sum_{i=0}^Lih_{I_f}$, sendo $h_{I_f}$ o histograma normalizado da imagem proveniente da fusão $I_f(i,j)$, e $L$ o número de frequência existente no histograma.
\subsection{Entropy}
\begin{equation}
He=-\sum_{i=1}^Lh_{I_f}(i)\log_2 h_{I_f}(i)
\end{equation}
\subsection{Cross Entropy}
A entropia cruzada das imagens fontes $I_1$ e $I_2$ e a imagem fundida $I_f$ é:
\begin{equation}
CE(I_1,I_2;I_f)=\frac{CE(I_1;I_f)+CE(I_2;I_f)}{2}
\end{equation}
onde $CE(I_1;I_f)=\sum_{i=1}^Lh_{I_f}(i)\log_2\left( \frac{h_{I_1}(i)}{h_{I_f}(i)}\right)$ e $CE(I_1;I_f)=\sum_{i=1}^Lh_{I_f}(i)\log_2\left( \frac{h_{I_1}(i)}{h_{I_f}(i)}\right)$
\subsection{Spatial frequency}
\begin{equation}
SF=\sqrt{RF^2+CF^2}
\end{equation}
onde, $RF=\sqrt{\frac{1}{MN}\sum_{x=1}^M\sum_{y=2}^N[I_f(x,y)-I_f(x,y-1)]^2}$ e $RF=\sqrt{\frac{1}{MN}\sum_{y=1}^N\sum_{x=2}^M[I_f(x,y)-I_f(x-1,y)]^2}$
\subsection{Fusion mutual information}
Se o histograma conjunto entre $I_1(x,y)$ e $I_f(x,y)$ é definido como $h_{I_1I_f}$ e entre $I_2(x,y)$ e $I_f(x,y)$ é definido como $h_{I_2I_f}$ então
\begin{equation}
FMI = MI_{I_1I_f}+MI_{I_2I_f}
\end{equation}
$$MI_{I_1I_f}= \sum_{i=1}^M\sum_{j=1}^N h{I_1I_f}(i,j)\log_2\left(\frac{h_{I_1I_f}(i,j)}{h_{I_1}(i,j)h_{I_f}(i,j)} \right)$$
$$MI_{I_2I_f}= \sum_{i=1}^M\sum_{j=1}^N h{I_2I_f}(i,j)\log_2\left(\frac{h_{I_2I_f}(i,j)}{h_{I_2}(i,j)h_{I_f}(i,j)} \right)$$
\subsection{Fusion quality index}
\begin{equation}
FQI = \sum_{w\in W}c(w)[\lambda(w)QI(I_1,I_f|w)+(1-\lambda(w))QI(I_2,I_f|w)] 
\end{equation}

Onde $\lambda(w)=\frac{\sigma_{I_1}^2}{\sigma_{I_1}^2+\sigma_{I_w}^2}$ computado sobre uma janela definida; $C(w)=max(\sigma_{I_1}^2,\sigma_{I_2}^2)$ sobre uma janela onde $c(x)$ é a normalização de $C(w)$ e $QI(I_1,I_f|w)$ é o índice de qualidade sobre a janela dado a imagem fonte e a imagem fundida.
\subsection{Fusion similarity metric}
\begin{equation}
FSM = \sum_{w\in W} sim(I_1,I_2,I_f|w)[QI(I_1,I_f|w)-QI(I_2,I_f|w)]+QI(I_2,I_f|w) 
\end{equation}
Onde
\begin{equation}
\text{sim}(I_1,I_2,I_f|w) = \left\{
\begin{array}{ccc}
    0   & \text{if} &  \frac{\sigma_{I_1I_f}}{\sigma_{I_1I_f}\sigma_{I_2I_f}} > 0  \\
    \frac{\sigma_{I_1I_f}}{\sigma_{I_1I_f}\sigma_{I_2I_f}}  & \text{if} &  0\le \frac{\sigma_{I_1I_f}}{\sigma_{I_1I_f}\sigma_{I_2I_f}} \le 1  \\
        1   & \text{if} &  \frac{\sigma_{I_1I_f}}{\sigma_{I_1I_f}\sigma_{I_2I_f}} > 1  \\
\end{array}
\right.,
\end{equation}
OBS: Ler o Método do jacobiano para descobrir a densidade!!!! 
\bibliographystyle{IEEEtran}
\bibliography{bibliografia}

\end{document}