\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}                   % AAB inserido
\usepackage[utf8]{inputenc}             % AAB inserido
\usepackage{rotating}                   % AAB inserido
%\usepackage[round,sort,nonamebreak]{natbib} % citação bibliográfica textual
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
%AAB
\DeclareMathOperator{\traco}{tr}
\begin{document}

\title{Paper Title*\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Bolsista Capes.}
}

\author{\IEEEauthorblockN{Anderson A. de Borba}
\IEEEauthorblockA{\textit{Dept. Engenharia Elétrica e Computação} \\
\textit{UPM - Universidade Presbiteriana Mackenzie}\\
IBMEC-SP\\
São Paulo, Brazil \\
anderson.borba@ibmec.edu.br}
\and
\IEEEauthorblockN{Maurício Marengoni}
\IEEEauthorblockA{\textit{Dept. Engenharia Elétrica e Computação} \\
\textit{UPM - Universidade Presbiteriana Mackenzie}\\
São Paulo, Brazil \\
mauricio.marengoni@mackenzie.br}
\and
\IEEEauthorblockN{\hspace{6cm} Alejandro C. Frery}
\IEEEauthorblockA{\textit{\hspace{6cm}Laboratório de Computação Científica e Análise Numérica} \\
\hspace{6cm}\textit{UFAL - Universidade Federal de Alagoas)}\\
\hspace{6cm} Maceió, Brazil \\
\hspace{6cm}acfrery@gmail.com}
}

\maketitle

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
\section{Modelagem estatística para dados PolSAR}
Os sistemas SAR totalmente polarimétricos transmitem pulsos de micro-ondas polarizados ortogonalmente e medem componentes ortogonais do sinal recebido. Para cada pixel, a medida resulta em uma matriz de coeficientes de espalhamento. Esses coeficientes são números complexos que descrevem no sistema SAR a transformação do campo eletromagnético transmitido para o campo eletromagnético recebido.

A transformação pode ser representada como
\begin{equation}
 \left[
\begin{array}{c}
	E_{h}^{r}   \\
	E_{v}^{r}    \\
\end{array}
\right]
 = \frac{e^{\hat{\imath} kr}}{r}\left[
\begin{array}{cc}
	S_{hh}   & S_{hv}   \\
	S_{vh}   & S_{vv}   \\
\end{array}
\right]
 \left[
\begin{array}{c}
	E_{h}^{t}   \\
	E_{v}^{t}    \\
\end{array}
\right],
\end{equation}
onde $k$ denota o número de onda, $\hat{\imath}$ é um número complexo e $r$ é a distância entre o radar e o alvo. O campo eletromagnético com componentes $E_{i}^{j}$ tem índice subscrito denotando a polarização horizontal ($h$) ou vertical ($v$),  enquanto o índice sobrescrito indica a onda recebida ($r$) ou transmitida ($t$). Definindo $S_{i,j}$ como os coeficientes de espalhamento complexo, tal que o índice $i$ e $j$ são associados com o recebimento com a transmissão das ondas, por exemplo, o coeficiente de espalhamento $S_{hv}$ está associado a onda transmitida na direção vertical ($v$) e recebida na direção horizontal ($h$).

Sendo conhecido cada coeficientes, a matriz de espalhamento complexa $\mathbf{S}$ é definida por
\begin{equation}
\mathbf{S} = \left[
\begin{array}{cc}
	S_{hh}   & S_{hv}   \\
	S_{vh}   & S_{vv}   \\
\end{array}
\right],
\end{equation}
e se o meio de propagação das ondas é recíproco, isto é, de uma maneira geral as propriedades de transmissão e recebimento de uma antena são idênticos, então usaremos o teorema da reciprocidade \cite{lp} para definir a matriz de espalhamento como sendo hermitiana, ou seja, a igualdade dos termos complexos $S_{hv}=S_{vh}$. Portanto, a matriz de espalhamento pode ser representada pelo vetor
\begin{equation}
\mathbf{S} = \left[
\begin{array}{c}
	S_{hh}     \\
    S_{hv}     \\
	S_{vv}     \\
\end{array}
\right].
\end{equation}

De acordo com \cite{good} e \cite{lee} esta distribuição pode modelar adequadamente o comportamento estatístico de $\mathbf{S}$. A hipótese de ser gaussiana e circular foi comprovada para dados SAR polarimétricos no artigo \cite{sarabendi}.   

A matriz de covariância associada a $\mathbf{S}$ definida por
\begin{equation}
	{\bf C_{{\bf S}}} = E[{\mathbf S}{\mathbf S}^H] = \left[
\begin{array}{cc}
	E[S_{hh}\overline{S_{hh}}]  & E[{S_{hh}}\overline{S_{hv}}]   \\
	E[S_{hv}\overline{S_{hh}}]  & E[{S_{vv}}\overline{S_{vv}}]  \\
\end{array}
\right]
\end{equation}
talque, $\overline{\cdot}$ denota o conjugado complexo. A matriz de covariância é hermitiana positiva definida e contém todas as informações necessárias para caracterizar o retroespalhamento, podemos consultar mais informações em \cite{mfp}. como $N(0,\sigma)$.
Cada componente do vetor $\mathbf{S}$ é complexo, com intuito de fixar notação reescrevemos,
\begin{equation}
\begin{array}{ccc}
	S_{hh} &=& R_{hh} + i I_{hh}    \\
    S_{vh} &=& R_{hv} + i I_{hv}   \\
	S_{vv} &=& R_{vv} + i I_{vv}.   \\
\end{array}
\end{equation}
Outra forma de representar o vetor $\mathbf{S}$ é realocar as partes reais e complexas de cada elemento em um vetor de dimensão $6$ onde cada entrada é representado por
\begin{equation}
\mathbf{S} = \left[
\begin{array}{c}
	R_{hh}     \\
    I_{hh}     \\
	R_{hv}     \\
	I_{hv}     \\
    R_{vv}     \\
	I_{vv}     \\
\end{array}
\right]
\end{equation}
e respeitam uma distribuição gaussiana complexa de média 0, a qual representamos 

A matriz de covariância considerando as 3 amostras $(S_{hh}, S_{hv}, S_{vv})$ complexas, por definição tem dimensão $6 \times 6$, e por hipótese é uma distribuição gaussiana circular complexa multivariada com média zero assumindo a forma de matricial,  
\begin{equation*}
\tiny
\mathbf{S} \mathbf{S}^H= \left[
\begin{array}{rrrrrr}
	R_{hh}^2       & 0            &R_{hh}R_{hv}  &-R_{hh}I_{hv} &R_{hh}R_{vv} &-R_{hh}I_{vv}    \\
    0              & I_{hh}^2     &I_{hh}        &I_{hv}I_{hv}  &I_{hh}R_{vv}  &I_{hh}I_{vv}   \\
	R_{hv}R_{hh}   & R_{hv}I_{hh} &R_{hv}^2      & 0            &R_{hv}R_{vv}  &-R_{hv}I_{vv}     \\
	-I_{hv}R_{hh}  & I_{hv}I_{hh} &0             &I_{hv}^2      &I_{hv}R_{vv}  &I_{hh}I_{vv} \\
    R_{vv}I_{vv}   & R_{vv}I_{vv} &R_{vv}R_{hv}  &R_{vv}I_{hv}  &R_{vv}^2      &0            \\
	-I_{vv}R_{hh}  & I_{vv}I_{vv} &-I_{vv}R_{hv} &I_{vv}I_{hv}  &0             &I_{vv}^2     \\
\end{array}
\right].
\end{equation*}

Na referência \cite{good} foi descrito a hipótese de ser uma distribuição gaussiana circular complexa multivariada com média zero , portanto, sendo $\mathbf{S}_{ij}= R_{ij}+ i I_{ij}$ é exigido que $R_{ij}$ e $I_{ij}$ com $j=h,v$ satisfaçam a hipótese descritas detalhadamente por 
\begin{itemize}
	\item[I-] $E[R_{ij}]=E[I_{ij}]=0,$
	\item[II-] $E[R_{ij}^2]=E[I_{ij}^2],$ 
	\item[II-] $E[R_{ij}I_{ij}]=0,$  
	\item[IV-] $E[R_{ij}R_{ij}]=E[I_{ij}I_{ij}],$ 
	\item[V-] $E[I_{ij}R_{ij}]=-E[R_{ij}I_{ij}],$
\end{itemize}
onde, $E[\cdot]$ denota o valor esperado. 

As hipóteses explicam o formato da matriz de covariância e veremos a analise das condições acima detalhadamente,
\begin{itemize}
	\item[I-] A condição $$E[R_{ij}]=E[I_{ij}]=0,$$ pode ser escrita por, 
	$$E[R_{hh}]=E[I_{hh}]=0,$$ 
	$$E[R_{hv}]=E[I_{hv}]=0,$$ 
  e $$E[R_{vv}]=E[I_{vv}]=0.$$	                               
	\item[II-] A condição (II) $$E[R_{ij}^2]=E[I_{ij}^2],$$ pode ser escrita,      $$R_{hh}^2=I_{hh}^2,$$ 
	$$R_{hh}^2=I_{hh}^2,$$ 
	e $$R_{vv}^2=I_{v}^2.$$
	\item[III-]A condição (III) $$E[R_{ij}I_{ij}]=0,$$ pode ser escrita, $$R_{hh}I_{hh}=0,
$$ $$R_{hv}I_{hv}=0,$$
e $$R_{vv}I_{vv}=0.$$ 
	\item[IV-] A condição (IV) $$E[R_{ij}R_{ij}]=E[I_{ij}I_{ij}],$$ pode ser escrita, $$R_{hh}R_{hv}=I_{hh}I_{hv},$$
	     $$R_{hh}R_{vv}=I_{hh}I_{vv},$$
	     $$R_{hv}R_{vv}=I_{hv}I_{vv},$$
	     $$R_{hv}R_{hv}=I_{hv}I_{hv},$$ 
	     $$R_{vv}R_{hv}=I_{vv}I_{hv},$$
	  e  $$R_{vv}R_{hv}=I_{vv}I_{hv}.$$ 
	\item[V-] A condição (V) 
	$$E[I_{ij}R_{ij}]=-E[R_{ij}I_{ij}],$$ pode ser escrita,
    $$I_{hv}R_{hv}=-R_{hh}I_{hv},$$
    $$I_{vv}R_{hh}=-R_{hh}I_{vv},$$
    $$I_{vv}R_{hv}=-R_{vv}I_{hv},$$
    $$I_{hv}R_{hh}=-R_{hh}I_{hv},$$
    $$I_{hh}R_{vv}=-R_{hh}I_{vv},$$
  e $$I_{hv}R_{vv}=-R_{hv}I_{vv}.$$ 
\end{itemize}

Nas imagens PolSAR serão consideradas três componentes para o vetor $\mathbf{S}=[S_{hh},S_{vh},S_{vv}]^T$ e a multiplicação de $\mathbf{s}=[S_{hh},S_{vh},S_{vv}]$ pelo seu conjugado transposto $\mathbf{S}=[S_{hh},S_{vh},S_{vv}]^H$, isto é, a hermitiana do vetor, 

\begin{equation}
\mathbf{s}\mathbf{s}^H = \left[
\begin{array}{c}
	S_{hh}      \\
        S_{vh}     \\
	S_{vv}      \\
\end{array}
\right]
\left[
\begin{array}{ccc}
	S_{hh}  & S_{vh}  & S_{vv}      \\
\end{array}
\right]^H = \left[S_{ij} \right]_{i,j=h,v}
\end{equation}

De acordo com \cite{good} a distribuição gaussiana complexa multivariada pode modelar adequadamente o comportamento estatístico de $\boldmath S$. Isto é chamado de {\it single-look complex PolSAR data representation} e podemos definir o vetor de espalhamento por $\mathbf{S}=[S_{hh},S_{hv},S_{vv}]^H$. 

Dados polarimétricos são usualmente sujeitados a um processo de várias visadas com o intuito de melhorar a razão entre o sinal e o seu ruído. Para esse fim, matrizes positivas definidas hermitianas estimadas são obtidas computando a média de $L$ visadas independentes de uma mesma cena. Resultando na matriz de covariância amostral estimada {\bf Z} conforme \cite{good, ade}
\begin{equation}
\begin{array}{ccc}
    \mathbf{Z}&=&\frac{1}{L}\displaystyle{\sum_{l=1}^{L} {\mathbf{s}_l}{\mathbf{s}_l}^H}, \\
\end{array}
\end{equation}
onde $\mathbf{s}_l$ com $l = 1, \dots, L$ é uma amostra de $\mathit{L}$ vetores complexos distribuídos como $\mathbf{S}$, assim a matriz de covariância amostral associada a $\mathbf{S}_l$, com $l=1,\dots,L$ denotam o espalhamento para cada visada $L$

Sendo $i=j$
\begin{equation}
\begin{array}{ccc}
\mathbf{S}_{ii}\overline{\mathbf{S}}_{ii}&=& (R_{ii}+iI_{ii})\overline{(R_{ii}+iI_{jj})} \\
\mathbf{S}_{ii}\overline{\mathbf{S}}_{ii}&=& (R_{ii}+iI_{ii})(R_{ii}-iI_{ii}) \\
\mathbf{S}_{ii}\overline{\mathbf{S}}_{ii}&=& R_{ii}^2+I_{ii}^2 \\
\end{array}
\end{equation}
e considerando $i \neq j$
\begin{equation}
\begin{array}{ccc}
\mathbf{S}_{ii}\overline{\mathbf{S}}_{ij}&=& (R_{ii}+iI_{ii})\overline{(R_{ij}+iI_{ij})} \\
\mathbf{S}_{ii}\overline{\mathbf{S}}_{ij}&=& (R_{ii}+iy_{ii})(I_{ij}-iI_{ij}) \\
\mathbf{S}_{ii}\overline{\mathbf{S}}_{ij}&=& (R_{ii}R_{ij}+I_{ii}I_{ij})+i(R_{ij}I_{ii}-R_{ii}I_{ij}) \\
\end{array}
\end{equation}
 Definindo,
 \begin{equation}
\begin{array}{ccc}
	  RC_{ij}&=&  R_{ii}R_{ij}+I_{ii}I_{ij} 
\end{array}
\end{equation}
e
\begin{equation}
\begin{array}{ccc}
	  IC_{ij}&=& R_{ij}I_{ii}-R_{ij}I_{ii}
\end{array}
\end{equation}

Sendo a variável randômica gaussiana complexa $\mathbf{C_{i,j}}=RC_{ij} + i IC_{ij}$, ou ainda, $\mathbf{C_{i,j}}=(R_{ii}R_{ij}+I_{ii}I_{ij}) + i(R_{ij}I_{ii}-R_{ij}I_{ii})$. Podemos escrever uma variável randômica gaussiana complexa $4-$variada $(R_{ii},R_{ij},I{ii},I_{ij})$.

De acordo com (\cite{good})
\begin{equation}
\mathbf{C} = \left[
\begin{array}{cc}
	E(X_iX_j)  & E(X_iY_j)  \\
	E(Y_iX_j)  & E(Y_iY_j)  \\
\end{array}
\right].
\end{equation}
Tal que
\begin{equation}
\mathbf{C} =
\left\{
\begin{array}{cc}
	\frac{1}{2}\left[
\begin{array}{cc}
	 1 & 0  \\
	 0 & 1  \\
\end{array}
	\right]\sigma^{2}_{j}  & \mbox{se}\quad i=j, \\
	& \\
	\frac{1}{2}\left[
\begin{array}{cc}
	\alpha_{ij} & -\beta_{ij}  \\
	 \beta_{ij} & \alpha_{ij}  \\
\end{array}
	\right]\sigma_j\sigma_k  & \mbox{se}\quad i\neq j.   \\
\end{array}
\right.
\end{equation}

\begin{sidewaystable}
	\centering
	\caption{Tabela}
\begin{tabular}{@{}lcccccc@{}} \toprule
	     &$R_{hh}$        & $I_{hh}$ & $R_{hv}$&$I_{hv}$                            &$R_{vv}$                           &$I_{vv}$ \\ \midrule
$R_{hh}$ &$\sigma_{hh}^2$ & 0                  &$\rho_{hh,hv}\sigma_{hh}\sigma_{hv}$ &$\eta_{hh,hv}\sigma_{hh}\sigma_{hv}$ & $\rho_{hh,vv}\sigma_{hh}\sigma_{vv}$&$\eta_{hh,vv}\sigma_{hh}\sigma_{vv}$  \\ 
	$I_{hh}$ & 0 & $\sigma_{hh}^2$ &$-\eta_{hh,hv}\sigma_{hh}\sigma_{hv}$ &$\rho_{hh,hv}\sigma_{hh}\sigma_{hv}$ &$-\eta_{hh,vv}\sigma_{hh}\sigma_{vv}$ &$\rho_{hh,vv}\sigma_{hh}\sigma_{vv}$  \\ 
	$R_{hv}$ &$\rho_{hh,hv}\sigma_{hh}\sigma_{hv}$   &$-\eta_{hh,hv}\sigma_{hh}\sigma_{hv}$  &$\sigma_{hv}^2$ &0 &$\rho_{hv,vv}\sigma_{hv}\sigma_{vv}$ &$\eta_{hv,vv}\sigma_{hv}\sigma_{vv}$  \\ 
	$I_{hv}$ &$\eta_{hh,hv}\sigma_{hh}\sigma_{hv}$  &$\rho_{hh,hv}\sigma_{hh}\sigma_{hv}$  &0 &$\sigma_{hv}^2$ &$-\eta_{hv,vv}\sigma_{hv}\sigma_{vv}$ &$\rho_{hv,vv}\sigma_{hv}\sigma_{vv}$ \\ 
	$R_{vv}$ &$\rho_{hh,vv}\sigma_{hh}\sigma_{vv}$  &$-\eta_{hh,vv}\sigma_{hh}\sigma_{vv}$  &$\rho_{hv,vv}\sigma_{hv}\sigma_{vv}$ &$-\eta_{hv,vv}\sigma_{hv}\sigma_{vv}$ & $\sigma_{vv}^2$ &0 \\ 
    $I_{vv}$ &$\eta_{hh,vv}\sigma_{hh}\sigma_{hv}$  &$\rho_{hh,vv}\sigma_{hh}\sigma_{vv}$  &$\eta_{hv,vv}\sigma_{hv}\sigma_{vv}$ &$\rho_{hv,vv}\sigma_{hv}\sigma_{vv}$ & 0 &$\sigma_{vv}^2$ \\ 	 \bottomrule
\end{tabular}
\end{sidewaystable}
\section{Funções de densidade}
\subsection{Função de densidade Wishart}
Para os canais $(hh)$, $(hv)$ e $(vv)$ vamos usar a distribuição Wishart (PDF) descrita por

\begin{equation}
    f_{\mathbf{Z}}(\mathbf{Z};\mathbf{\Sigma_{s}},L)=\frac{L^{mL}|\mathbf{Z}|^{L-m}}{|\mathbf{\Sigma_{s}}|^{L}\Gamma_m(L)} \exp(-L\traco(\mathbf{\Sigma_{s}}^{-1}\mathbf{Z})), \\
\end{equation} 

onde, $\traco(\cdot)$ é o operador traço de uma matriz, $\Gamma_m(L)$ é uma função Gamma multivariada definida por
\begin{equation}\label{cap_acf_10}
	\Gamma_m(L)=\pi^{\frac{1}{2}m(m-1)} \prod_{i=0}^{m-1}\Gamma(L-i) \\
\end{equation}
e $\Gamma(\cdot)$ é a função Gamma. Podemos afirmar que $\mathbf{Z}$ é distribuído como uma distribuição Wishart denotando por $\mathbf{Z}\sim W(\mathbf{\Sigma_{s}}, L)$ e satisfazendo $E[\mathbf{Z}]=\mathbf{\Sigma_{s}}$. Sem perda de generalidade para o texto vamos usar o simbolo $\mathbf{\Sigma}$ em detrimento a $\mathbf{\Sigma_{s}}$ para representar a matriz de covariância associada a $\mathbf{S}$.

\subsection{Função de densidade para a magnitude do produto $\mathbf{S}_i$ e $\mathbf{S}_j$}
A magnitude do produto $\mathbf{S}_i$ e $\mathbf{S}_j$ é uma importante medida para as imagem SAR polarimétrica. Definimos a magnitude normalizada por 

\begin{equation}
	\xi = \frac{\left|\frac{1}{L} \sum_{k=1}^L\mathbf{S}_i(k)\mathbf{S}_j^H(k) \right|}{\sqrt{E[|\mathbf{S}_i|^2]E[|\mathbf{S}_i|^2]}}=\frac{g}{h}.
\end{equation}
onde é definido por $g=|\mathbf{S}_i\mathbf{S}_j^H|$ e $h=\sqrt{E[|\mathbf{S}_i|^2]E[|\mathbf{S}_i|^2]}$.

A PDF de $\xi$

\begin{equation}
\begin{array}{ccc}
	f(\xi)&=&\frac{4L^{L+1}\xi^L}{\Gamma(L)(1-|\rho|^2)}I_0\left(\frac{2|\rho|L\xi}{1-|\rho|^2}\right)K_{L-1}\left(\frac{2L\xi}{1-|\rho|^2}\right).
		\end{array}
\end{equation}
onde $I_0$ e $K_{L-1}$ são funções de Bessel modificadas.

A PDF para a magnitude não normalizada $g$ pode ser obtida pelas equações acima:
\begin{equation}
\begin{array}{ccc}
	f(g)&=&\frac{4L^{L+1}g^L}{\Gamma(L)(1-|\rho|^2)h^{L+1}}I_0\left(\frac{2|\rho|Lg}{(1-|\rho|^2)h}\right)K_{L-1}\left(\frac{2Lg}{(1-|\rho|^2)h}\right).
		\end{array}
\end{equation}
\subsection{Função de densidade para cada canal complexo}

Sendo $(R_{ii}, R_{ij})\sim N2(0, C_{ij})$ podemos observar na tabela anterior que 
\begin{equation}
C_{ij}=\left[
\begin{array}{cc}
	\sigma_{ii}^2   &  \rho_{ii,ij}\sigma_{ii}\sigma_{ij}  \\
	\rho_{ii,ij}\sigma_{ii}\sigma_{ij} & \sigma_{ij}^2   \\
\end{array}
\right],
\end{equation}

A pdf para esta distribuição normal é:

\begin{equation}
\begin{array}{ccc}
	f_{Z_{R_{ii}R_{ij}}}(z)&=&\frac{1}{\pi\sigma_{ii}\sigma_{ij}\sqrt{1-\rho_{ii,ij}^2}}\exp\left(\frac{\rho_{ii,ij}z}{\sigma_{ii}\sigma_{ij}(1-\rho_{ii,ij})^2}\right)\\
	&&K_0\left(\frac{|z|}{\sigma_{ii}\sigma_{ij}(1-\rho_{ii,ij})^2}\right).
\end{array}
\end{equation}


Sendo $(I_{ii}, I_{ij})\sim N2(0, C_{ij})$ podemos observar na tabela anterior que 
\begin{equation}
C_{ij}=\left[
\begin{array}{cc}
	\sigma_{ii}^2   &  \rho_{ii,ij}\sigma_{ii}\sigma_{ij}  \\
	\rho_{ii,ij}\sigma_{ii}\sigma_{ij} & \sigma_{ij}^2   \\
\end{array}
\right],
\end{equation}
\begin{equation}
\begin{array}{ccc}
	f_{Z_{R_{ii}R_{ij}}}(z)&=&\frac{1}{\pi\sigma_{ii}\sigma_{ij}\sqrt{1-\rho_{ii,ij}^2}}\exp\left(\frac{\rho_{ii,ij}z}{\sigma_{ii}\sigma_{ij}(1-\rho_{ii,ij})^2}\right)\\
	&&K_0\left(\frac{|z|}{\sigma_{ii}\sigma_{ij}(1-\rho_{ii,ij})^2}\right).
\end{array}
\end{equation}

Definindo o funcional $\Theta(z;\sigma_p,\sigma_q,\gamma)$
\begin{equation}
\begin{array}{ccc}
	\Theta(z;\sigma_p,\sigma_q,\gamma)&=&\frac{1}{\pi\sigma_p\sigma_q\sqrt{1-\gamma^2}}\exp\left(\frac{\gamma z}{\sigma_p\sigma_q(1-\gamma)^2}\right)\\
	&&K_0\left(\frac{|z|}{\sigma_p\sigma_q(1-\gamma)^2}\right).
\end{array}
\end{equation}
onde,  $\sigma_p,\sigma_q,\gamma$ são  parâmetros da função.
\subsection{distribuição conjunta para  $(R_{ii}, R_{ij})\sim N2(0, C_{ij})$}
Sendo $(R_{ii}, R_{ij},I_{ii}, I_{ij})\sim N4(0, C_{ii,ij})$ podemos observar na tabela anterior que 
\begin{equation}
C_{ii,ij}=\left[
\begin{array}{cccc}
	\sigma_{ii}^2   &  \rho_{ii,ij}\sigma_{ii}\sigma_{ij} & 0&\eta_{ii,ij}\sigma_{ii}\sigma_{ij}\\
	\rho_{ii,ij}\sigma_{ii}\sigma_{ij} & \sigma_{ij}^2  & -\eta_{ii,ij}\sigma_{ii}\sigma_{ij}&0 \\
	0&-\eta_{ii,ij}\sigma_{ii}\sigma_{ij}&\sigma_{ii}^2&\rho_{ii,ij}\sigma_{ii}\sigma_{ij}\\
	\eta_{ii,ij}\sigma_{ii}\sigma_{ij}&0&\rho_{ii,ij}\sigma_{ii}\sigma_{ij}&\sigma_{ij}^2\\
\end{array}
\right],
\end{equation}
Realizar a transformação 
\begin{equation}
\left[
\begin{array}{ccc}
	 Z = R_{ii}R_{ij}+I_{ii}I_{ij} \\
	 U_1 = R_{ii}\\
	 U_2 = R_{ij}\\
	 U_3 = I_{ii}\\
\end{array}
\right],
\end{equation}
\section{Deteção de borda}
\subsection{Detecção de bordas nas bandas $I_{hh}$, $I_{hv}$ e $I_{vv}$}
Dados polarimétricos são usualmente sujeitados a um processo de várias visadas com o intuito de melhorar a razão entre o sinal e o seu ruído. Para esse fim, matrizes positivas definidas hermitianas estimadas são obtidas computando a média de $L$ visadas independentes de uma mesma cena. Resultando na matriz de covariância amostral estimada {\bf Z} conforme \cite{good, ade}
\begin{equation}
\begin{array}{ccc}
    \mathbf{Z}&=&\frac{1}{L}\displaystyle{\sum_{i=1}^{L} {\mathbf{s}_i}{\mathbf{s}_i}^H}, \\
\end{array}
\end{equation}
onde $\mathbf{s}_i$ com $i = 1, \dots, L$ é uma amostra de $\mathit{L}$ vetores complexos distribuídos como $\mathbf{s}$, assim a matriz de covariância amostral associada a $\mathbf{s}_i$, com $i=1,\dots,L$ denotam o espalhamento para cada visada $L$ seguindo uma distribuição complexas de Wishart. 

Sendo agora $\mathbf{\Sigma_{s}}$ e $L$ parâmetros conhecidos a função densidade de probabilidade da distribuição Wishart por  
%
\begin{equation}
    f_{\mathbf{Z}}(\mathbf{Z};\mathbf{\Sigma_{s}},L)=\frac{L^{mL}|\mathbf{Z}|^{L-m}}{|\mathbf{\Sigma_{s}}|^{L}\Gamma_m(L)} \exp(-L\traco(\mathbf{\Sigma_{s}}^{-1}\mathbf{Z})), \\
\end{equation}
onde, $\traco(\cdot)$ é o operador traço de uma matriz, $\Gamma_m(L)$ é uma função Gamma multivariada definida por
\begin{equation}
	\Gamma_m(L)=\pi^{\frac{1}{2}m(m-1)} \prod_{i=0}^{m-1}\Gamma(L-i) \\
\end{equation}
e $\Gamma(\cdot)$ é a função Gamma.

\subsection{Método da máxima verosimilhança}

O conceito de verossimilhança é importante em estatística, pois descreve de maneira precisa as informações sobre os parâmetros do modelo estatístico que representa os dados observados. De maneira geral, a estimativa por máxima verossimilhança (\textbf{MLE}) é um método que, tendo um conjunto de dados e um modelo estatístico, estima os valores dos parâmetros do modelo estatísticos com intuito de maximizar uma função de probabilidade dos dados. 

Suponha $\mathbf{X}=(X_1,X_2,\dots,X_n)^T$ um vetor randômico distribuído de acordo com a $\mathbf{p.d.f}$ função densidade de probabilidade $f(\mathbf{x},\mathbf{\theta})$ com parâmetros $\mathbf{\theta}=(\theta_1,\dots,\theta_d)^T$ no espaço dos parâmetros $\Theta$. Definimos  a \textbf{função de verossimilhança} cuja amostra é 
\begin{equation}
    L(\theta;\mathbf{X}) = \prod_{i=1}^{n}f(x_i;\theta), \\
\end{equation}
e a função logarítmica de verossimilhança a qual podemos chamar de  \textbf{função de log-verossimilhança} é a soma
\begin{equation}
	l(\theta;\mathbf{X})= \ln(L(\theta;\mathbf{X})) = \sum_{i=1}^{n}\ln(f(x_i;\theta)). \\
\end{equation}

Podemos definir o método da \textbf{estimativa de máxima verossimilhança} (\textbf{MLE}) de $\theta$ como sendo o vetor $\widehat{\theta}$ tal que $L(\widehat{\theta};\mathbf{x})\ge L(\theta;\mathbf{x})$ para todo $\theta$ no espaço dos parâmetros $\Theta$. De maneira simplificada a \textbf{estimativa de máxima verossimilhança} pode ser escrita por 
\begin{equation}
    \widehat{\theta}= \text{arg}\,\max\limits_{\theta\in\Theta}L(\theta;\mathbf{x}),\\
\end{equation}
ou de maneira similar 
\begin{equation}
    \widehat{\theta}= \text{arg}\,\max\limits_{\theta\in\Theta}l(\theta;\mathbf{x}).\\
\end{equation}

\subsection{Estimativa de máxima verossimilhança aplicada a distribuíção Wishart}

Nesta seção vamos usar o método de máxima verossimilhança aplicado na distribuição Wishart. Suponha $\mathbf{Z}=(\mathbf{Z}_1,\mathbf{Z}_2,\dots,\mathbf{Z}_N)^T$ um vetor randômico distribuído de acordo com a $\mathbf{p.d.f}$ função densidade de probabilidade (\ref{cap_acf_9}) com parâmetros $\Sigma=\{\mathbf{\Sigma_A}, \mathbf{\Sigma_B\}}$ e $L$.

A \textbf{função de verossimilhança} da amostra descrita por (\ref{cap_acf_13}) é dada pela equação produtório das funções de densidade respectivamente associadas a cada amostra
\begin{equation}
	L(j)=\prod_{k=1}^{j}f_{\mathbf{Z}}(\mathbf{Z}_{k}^{'};\mathbf{\Sigma_{A}},L) \prod_{k=j+1}^{N}f_{\mathbf{Z}}(\mathbf{Z}_{k}^{'};\mathbf{\Sigma_{B}},L), \\
\end{equation}
onde $\mathbf{Z}_{k}^{'}$ é uma possível aproximação da matriz randômica descrita em (\ref{cap_acf_13}).

Usando a equação (\ref{cap_acf_12}) e propriedades de logaritmos natural teremos para cada termo do produtório (\ref{cap_acf_18}) uma \textbf{função de log-verossimilhança}. Assim, encontramos uma função dependente de $j$
\begin{equation}
\begin{array}{rcl}
	l(j)=\ln L(j)&=&\sum_{k=1}^{j}\ln f_{\mathbf{Z}}(\mathbf{Z}_{k}^{'};\mathbf{\Sigma_{A}},L)\\
	             &+&\sum_{k=j+1}^{N}\ln f_{\mathbf{Z}}(\mathbf{Z}_{k}^{'};\mathbf{\Sigma_{B}},L).
\end{array}
\end{equation}

Nesse momento, podemos realizar  manipulações algébricas na função densidade de probabilidade em cada termo do somatório conforme (\ref{cap_acf_12}) e substituir nas duas parcelas da equação (\ref{cap_acf_19}) levando em consideração que as duas amostras são diferentes, desta forma
\begin{equation}
\begin{array}{lll}
	l(j)&=&N\left[mL\ln{\left(L\right)}-\ln{\left(\Gamma_m(L)\right)}\right]\\
	&-& L\left[j\ln{\left(|\mathbf{\Sigma_{A}}|\right)}+(N-j)\ln{\left(|\mathbf{\Sigma_{B}}|\right)}\right] \\
	&+&(L-m)\sum_{k=1}^{N}\ln{\left(|\mathbf{Z}_{k}^{'}|\right)}\\
	&-&L\left[\sum_{k=1}^{j}tr(\mathbf{\Sigma_{A}}^{-1}\mathbf{Z}_{k}^{'})+ \sum_{k=j+1}^{N}tr(\mathbf{\Sigma_{B}}^{-1}\mathbf{Z}_{k}^{'})\right]. \\
\end{array}
\end{equation}

A matriz $\Sigma$ pode ser encontrada usando o estimador de máxima verossimilhança denotado por $\widehat{\Sigma}$ de acordo com a referência \cite{good}. A equação (\ref{cap_acf_20}) representa duas estimativa para a matriz de covariância $\Sigma$ que dependem da posição $j$
\begin{equation}
\widehat{\Sigma_{I}}(j) = \left\{
\begin{array}{lc}
	j^{-1}\sum_{k=1}^{j}\mathbf{Z}_{k}  & \mbox{se}\quad I=A,  \\
        (N-j)^{-1}\sum_{k=j+1}^{N}\mathbf{Z}_{k} & \mbox{se}\quad I=B. \\
\end{array}
\right.
\end{equation}

Usando a equação (\ref{cap_acf_20}) podemos substituir na equação acima e continuar a manipulação algébrica
portanto, 
\begin{equation}
\begin{array}{rcl}
	l(j)&=&N\left[-mL(1-\ln{\left(L\right)})-\ln{\left(\Gamma_m(L)\right)}\right]\\
	&-&L\left[j\ln{\left(|\mathbf{\widehat{\Sigma}}_{A}(j)|\right)} +(N-j)\ln{\left(|\mathbf{\widehat{\Sigma}}_{B}(j)|\right)}\right], \\
	&+&(L-m)\sum_{k=1}^{N}\ln{\left(|\mathbf{Z}_{k}^{'}|\right)}. \\
\end{array}
\end{equation}

O estimador de máxima verossimilhança $\widehat{\jmath}_{ML}$ é uma evidência de borda por representar uma aproximação da transição de região e pode ser calculado pelo método de maximização
\begin{equation}
\begin{array}{rcl}
	\widehat{\jmath}_{ML}&=&\text{arg}\max\limits_{j}l(j).  \\
\end{array}
\end{equation}
\subsection{Estimativa de máxima verossimilhança aplicada na densidade de magnitude do produto $\mathbf{S}_i$ e $\mathbf{S}_j$}
A \textbf{função de verossimilhança} da amostra descrita por (\ref{}) é dada pela produtório das funções de densidade respectivamente associadas a cada amostra gerando a a função $L$
\begin{equation}
	L(j)=\prod_{k=1}^{j}p(\mathbf{Z}_{k}^{'}) \prod_{k=j+1}^{N}p(\mathbf{Z}_{k}^{'}), \\
\end{equation}
onde $\mathbf{Z}_{k}^{'}$ é uma possível aproximação da matriz randômica descrita em (ref).
Usando a equação (\ref) e propriedades de logaritmos natural teremos para cada termo do produtório (\ref{cap_acf_18}) uma \textbf{função de log-verossimilhança}. Assim, encontramos uma função dependente de $j$
\begin{equation}
\begin{array}{rcl}
	l(j)=\ln L(j)&=&\sum_{k=1}^{j}\ln p(\mathbf{Z}_{k}^{'})\\
	             &+&\sum_{k=j+1}^{N}\ln p(\mathbf{Z}_{k}^{'}).
\end{array}
\end{equation}

Substituindo a função de densidade e realizando as operações algébricas teremos
\begin{equation}
\begin{array}{rcl}
	l(j)=\ln L(j)&=&N\ln(4)+N(L+1)\ln(L)\\
	             &-&N\ln(\Gamma(L))-N\ln(1-|\rho|^2)\\
	             &+&\sum_{k=1}^{j}\ln(\xi)+\sum_{k=j+1}^{N}\ln(\xi) \\
	             &+&\sum_{k=1}^{j}I_0\left(\frac{2|\rho|L\xi}{1-|\rho|^2}\right) +\sum_{k=j+1}^{N} I_0\left(\frac{2|\rho|L\xi}{1-|\rho|^2}\right)        \\
	             &+&\sum_{k=1}^{j}K_{L-1}\left(\frac{2L\xi}{1-|\rho|^2}\right) +\sum_{k=j+1}^{N} K_{L-1}\left(\frac{2L\xi}{1-|\rho|^2}\right)         \\
\end{array}
\end{equation}



\begin{equation}
\begin{array}{ccc}
	f(\xi)&=&\frac{4L^{L+1}\xi^L}{\Gamma(L)(1-|\rho|^2)}I_0\left(\frac{2|\rho|L\xi}{1-|\rho|^2}\right)K_{L-1}\left(\frac{2L\xi}{1-|\rho|^2}\right).
		\end{array}
\end{equation}
\subsection{Função de densidade para a magnitude do produto $\mathbf{S}_i$ e $\mathbf{S}_j$}
A magnitude do produto $\mathbf{S}_i$ e $\mathbf{S}_j$ é uma importante medida para as imagem SAR polarimétrica. Definimos a magnitude normalizada por 

\begin{equation}\label{eq_06}
	\xi = \frac{\left|\frac{1}{L} \sum_{k=1}^L\mathbf{S}_i(k)\mathbf{S}_j^H(k) \right|}{\sqrt{E[|\mathbf{S}_i|^2]E[|\mathbf{S}_i|^2]}}=\frac{g}{h}.
\end{equation}
onde é definimos $g=|\mathbf{S}_i\mathbf{S}_j^H|$ e $h=\sqrt{E[|\mathbf{S}_i|^2]E[|\mathbf{S}_i|^2]}$.

A PDF para a variável  $\xi$ foi obtida no artigo (\cite{lee})
\begin{equation}\label{eq_07}
\begin{array}{ccc}
	f(\xi)&=&\frac{4L^{L+1}\xi^L}{\Gamma(L)(1-|\rho|^2)}I_0\left(\frac{2|\rho|L\xi}{1-|\rho|^2}\right)K_{L-1}\left(\frac{2L\xi}{1-|\rho|^2}\right).
		\end{array}
\end{equation}
onde $I_0$ e $K_{L-1}$ são funções de Bessel modificadas.

A PDF para a magnitude não normalizada $g$ pode ser obtida pelas equações (\ref{eq_07}) usando (\ref{eq_06}),
\begin{equation}
\begin{array}{ccc}\label{eq_08}
	f(g)&=&\frac{4L^{L+1}g^L}{\Gamma(L)(1-|\rho|^2)h^{L+1}}I_0\left(\frac{2|\rho|Lg}{(1-|\rho|^2)h}\right)K_{L-1}\left(\frac{2Lg}{(1-|\rho|^2)h}\right).
		\end{array}
\end{equation}

\subsection{Estimativa de máxima verossimilhança aplicada na densidade de magnitude do produto $\mathbf{S}_i$ e $\mathbf{S}_j$}
As funções de verossimilhança das amostras descritas por (\ref{eq_10}) e (\ref{eq_11}) serão usadas juntamente com a função de densidade (pdf) para a magnitude do produto $\mathbf{S}_i$ e $\mathbf{S}_j$. Partindo de 
\begin{equation*}
\begin{array}{rcl}
	l(j)=\ln L(j)&=&\sum_{k=1}^{j}\ln f_{\mathbf{Z}}(\mathbf{Z}_{k}^{'};\mathbf{\Sigma_{A}},L)\\
	             &+&\sum_{k=j+1}^{N}\ln f_{\mathbf{Z}}(\mathbf{Z}_{k}^{'};\mathbf{\Sigma_{B}},L).
\end{array}
\end{equation*}
e usando a distribuição (\ref{eq_08}) para duas amostras diferentes $A$ e $B$ teremos,
\begin{equation}
\begin{array}{rcl}
	l(j)=\ln L(j)&=&N\ln(4)+N(L+1)\ln(L)\\
	             &-&N\ln(\Gamma(L))-N\ln(1-|\rho|^2)\\
	             &+&L\sum_{k=1}^{j}\ln(g_k)+L\sum_{k=j+1}^{N}\ln(g_k) \\
	             &-&(L+1)\sum_{k=1}^{j}\ln(h_k)\\
	             &-&(L+1)\sum_{k=j+1}^{N}\ln(h_k) \\
	             &+&\sum_{k=1}^{j}\ln \left(I_0\left(\frac{2|\rho|L\xi}{1-|\rho|^2}\right)\right)\\ 
	             &+&\sum_{k=j+1}^{N} \ln \left(I_0\left(\frac{2|\rho|L\xi}{1-|\rho|^2}\right)\right)        \\
	             &+&\sum_{k=1}^{j}\ln \left(K_{L-1}\left(\frac{2L\xi}{1-|\rho|^2}\right)\right)\\
	             &+&\sum_{k=j+1}^{N}\ln \left( K_{L-1}\left(\frac{2L\xi}{1-|\rho|^2}\right)\right)         \\
	             
\end{array}
\end{equation}


\section{Métodos de fusão de evidências de bordas}
\subsection{Média simples}
O método de fusão com média simples propõe que a simples média das evidências de bordas consiste em um método de fusão de evidências de bordas. A médias das imagens pode ser calculada
\begin{equation}
	IF(x,y)=\frac{1}{nc}\sum_{i=1}^{nc}IE_i(x,y)
\end{equation}
onde $nc$ é o número de canais a serem utilizadas na fusão de evidências. Podemos obter mais detalhes na referência (\cite{mit}).
\subsection{Pincipal component analysis - (PCA) }
Esta seção é baseada na referência (\cite{n_r}) e (\cite{mit}). O método de fusão baseado no PCA pode ser descrito nos seguintes passos:
\begin{itemize}
\item[-] Organizar os dados de forma a ter cada imagem em um vetor coluna formando uma matriz $Z$ de dimensão $l\times nc$, onde $l=mn$, representa a multiplicação das $m$ linhas e $n$ colunas das matrizes a serem utilizadas na fusão.
\item[-] Calcule a média dos elementos dessas colunas gerando um vetor de dimesão $1\times nc$.
\item[-] Subtrai a média de cada coluna da matriz $Z$. Resultando em uma matriz $X$ de mesma dimensão de $Z$. 
\item[-] Ache a matriz de covariância $C$ proveniente de $X$, para isso calculamos $C=XX^T$.
\item[-] Calcule os autovalores $\Lambda$ e os autovetores $D$ e ordene esses vetores em ordem decrescente. A matriz gerada pelos autovalores na diagonal e autovetores colocados em coluna conforme a ordenação, as mesmas tem dimensão $nc\times nc$.
\item[-] Compute as componentes $P_i=\frac{V(i)}{\sum_{i=1}^l V(i)}$ com $i=1,\cdots nc$.
\item[-] Realizamos a fusão $IF=\sum_{i=1}^{nc}P_iIE(x,y)$. Lembrando que o $\sum_{i=1}^{nc}P_i=1$.  
\end{itemize}
\subsection{Transformada wavelet estacinária - SWT} Esta seção também é baseada na referência (\cite{n_r}). O método de fusão SWT pode ser descrito nos seguintes passos
\begin{itemize}
\item[-] Calcule a decomposição SWT obtendo $L_{HH}$, $L_{HL}$, $L_{LH}$ e $L_{LL}$ para cada canal.
\item[-] Nas decomposições $L_{HH}$ é realizada a média aritmética de cada canal, e nas decomposições $L_{HL}$, $L_{LH}$ e $L_{LL}$, é encontrado o máximo entre cada canal, restando uma nova decomposição $\bar{L}_{HH}$, $\bar{L}_{HL}$, $\bar{L}_{LH}$ e $\bar{L}_{LL}$. Ou seja, realizando esse processo foi realizado a fusão de informações.
\item[-] Realizando a transformação inversa das wavelet e obtemos a imagem com a fusão das evidências.  
\end{itemize}
\subsection{Estatística ROC}
O método Estatística ROC esta proposto e descrito em detalhes nas referências \cite{gs} e \cite{fawcett}. O método descreve um método estatístico para obter informações de maneira automática em diversas imagens, no nosso deste artigo diversos canais. Podemos descreve os método no seguinte processo:
\begin{itemize}
\item[-] Obter as evidência de bordas nos canais aplicando o método descrito nesse artigo. Armazene essas evidências de bordas em $E_i$, com $i=1,\cdot,nc$ de maneira binária.
\item[-] Depois desse passo, defina uma matriz de frequência de bordas $V$. A matriz  $V$ é gerada somando as evidencias de bordas $E_i$.
\item[-] Utilize limiares variando de $t=1,\dots,nc$ gerando matrizes $M_t$. 
\item[-] Faça a comparação de $M_t$ fixado com todas as $E_i$ e  encontre a matriz de confusão para gerar a curva ROC. O ponto da curva ROC gerado que se aproximar (calculando a distância euclidiana ) da linha diagnóstico terá seu limiar considerado o ótimo.
\item[-] A matriz $M_t$ que corresponde a esse limiar é a fusão de evidências de bordas.
\end{itemize}
  
\section{Métricas}
\subsection{RMSE - Root mean square}
\begin{equation}
	RMSE=\sqrt{\frac{1}{MN}\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j)-I_f(i,j))^2}.  \\
\end{equation}
\subsection{MAE - Root Mean absolute}
\begin{equation}
	MAE=\frac{1}{MN}\sum_{i=1}^M\sum_{j=1}^N\left|I_r(i,j)-I_f(i,j)\right|.  \\
\end{equation}
\subsection{Percent fit error}
\begin{equation}
	PFE=\frac{norm(I_r-I_f)}{norm(I_r)}*100.  \\
\end{equation}
\subsection{Signal to noise ratio}
\begin{equation}
SRN = 20log_{10}\left(\frac{\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j))^2}{\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j)-I_f(i,j))^2}\right)
\end{equation}
\subsection{Peak signal to noise ratio}
\begin{equation}
PSRN = 20log_{10}\left(\frac{L^2}{\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j)-I_f(i,j))^2}\right)
\end{equation}
Aqui $L$ é o número de níveis de cinza na imagem. 
\subsection{Correlaçao}
\begin{equation}
CORR = \frac{2C_{rf}}{C_r+Cf}
\end{equation}
Onde $$C_r= \sum_{i=1}^M\sum_{j=1}^N(I_r(i,j))^2,$$ $$C_f=\sum_{i=1}^M\sum_{j=1}^N(I_f(i,j))^2,$$ $$C_{rf}=\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j)I_f(i,j)),$$
Aqui $L$ é o número de níveis de cinza na imagem. 

\subsection{Mutual information}
\begin{equation}
MI = \sum_{i=1}^M\sum_{j=1}^N h_{I_rI_f}(i,j){log_2\left(\frac{h_{I_rI_f}(i,j)}{h_{I_r}(i,j)h_{I_f}(i,j)}\right)}
\end{equation}
\subsection{Universal quality index}
\begin{equation}
QI=\frac{4\sigma_{I_rI_f}(\nu_{I_r}+\nu_{I_f})}{(\sigma_{I_r}^2+\sigma_{I_f}^2)(\nu_{I_r}^2+\nu_{I_f}^2)}
\end{equation}
onde 
$$\nu_{I_r}=\frac{1}{MN}\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j))^2,$$
$$\nu_{I_f}=\frac{1}{MN}\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j))^2,$$ $$\sigma_{I_r}^2=\frac{1}{MN-1}\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j)-\mu_{I_r})^2,$$
$$\sigma_{I_f}^2   =\frac{1}{MN-1}\sum_{i=1}^M\sum_{j=1}^N(I_f(i,j)-\mu_{I_f})^2$$ e
$$\sigma_{I_rI_f}^2=\frac{1}{MN-1}\sum_{i=1}^M\sum_{j=1}^N(I_r(i,j)-\mu_{I_r})(I_f(i,j)-\mu_{I_f})$$ 
\subsection{Measure of structural similarity}
\begin{equation}
SSIM=\frac{(2\nu_{I_r}\nu_{I_r}+C_1)(2\sigma_{I_rI_f}+C_2)}{(\mu_{I_r}^2+\nu_{I_f}^2+C_1)(\sigma_{I_r}^2+\sigma_{I_f}^2+C_2)}
\end{equation}
onde $C_1$ é uma constante que é incluída para evitar a instabilidade quando $(\mu_{I_r}^2+\nu_{I_f}^2+C_1)$ e $C_2$ é uma constante que é incluída para evitar a instabilidade quando $(\sigma_{I_r}^2+\sigma_{I_f}^2+C_2)$ é perto de zero.
\subsection{Standard deviation}
\begin{equation}
\sigma=\sqrt{\sum_{i=1}^L(i-\bar{i})^2h_{I_f}(i)}
\end{equation}
onde $\bar{i}=\sum_{i=0}^Lih_{I_f}$, sendo $h_{I_f}$ o histograma normalizado da imagem proveniente da fusão $I_f(i,j)$, e $L$ o número de frequência existente no histograma.
\subsection{Entropy}
\begin{equation}
He=-\sum_{i=1}^Lh_{I_f}(i)\log_2 h_{I_f}(i)
\end{equation}
\subsection{Cross Entropy}
A entropia cruzada das imagens fontes $I_1$ e $I_2$ e a imagem fundida $I_f$ é:
\begin{equation}
CE(I_1,I_2;I_f)=\frac{CE(I_1;I_f)+CE(I_2;I_f)}{2}
\end{equation}
onde $CE(I_1;I_f)=\sum_{i=1}^Lh_{I_f}(i)\log_2\left( \frac{h_{I_1}(i)}{h_{I_f}(i)}\right)$ e $CE(I_1;I_f)=\sum_{i=1}^Lh_{I_f}(i)\log_2\left( \frac{h_{I_1}(i)}{h_{I_f}(i)}\right)$
\subsection{Spatial frequency}
\begin{equation}
SF=\sqrt{RF^2+CF^2}
\end{equation}
onde, $RF=\sqrt{\frac{1}{MN}\sum_{x=1}^M\sum_{y=2}^N[I_f(x,y)-I_f(x,y-1)]^2}$ e $RF=\sqrt{\frac{1}{MN}\sum_{y=1}^N\sum_{x=2}^M[I_f(x,y)-I_f(x-1,y)]^2}$
\subsection{Fusion mutual information}
Se o histograma conjunto entre $I_1(x,y)$ e $I_f(x,y)$ é definido como $h_{I_1I_f}$ e entre $I_2(x,y)$ e $I_f(x,y)$ é definido como $h_{I_2I_f}$ então
\begin{equation}
FMI = MI_{I_1I_f}+MI_{I_2I_f}
\end{equation}
$$MI_{I_1I_f}= \sum_{i=1}^M\sum_{j=1}^N h{I_1I_f}(i,j)\log_2\left(\frac{h_{I_1I_f}(i,j)}{h_{I_1}(i,j)h_{I_f}(i,j)} \right)$$
$$MI_{I_2I_f}= \sum_{i=1}^M\sum_{j=1}^N h{I_2I_f}(i,j)\log_2\left(\frac{h_{I_2I_f}(i,j)}{h_{I_2}(i,j)h_{I_f}(i,j)} \right)$$
\subsection{Fusion quality index}
\begin{equation}
FQI = \sum_{w\in W}c(w)[\lambda(w)QI(I_1,I_f|w)+(1-\lambda(w))QI(I_2,I_f|w)] 
\end{equation}

Onde $\lambda(w)=\frac{\sigma_{I_1}^2}{\sigma_{I_1}^2+\sigma_{I_w}^2}$ computado sobre uma janela definida; $C(w)=max(\sigma_{I_1}^2,\sigma_{I_2}^2)$ sobre uma janela onde $c(x)$ é a normalização de $C(w)$ e $QI(I_1,I_f|w)$ é o índice de qualidade sobre a janela dado a imagem fonte e a imagem fundida.
\subsection{Fusion similarity metric}
\begin{equation}
FSM = \sum_{w\in W} sim(I_1,I_2,I_f|w)[QI(I_1,I_f|w)-QI(I_2,I_f|w)]+QI(I_2,I_f|w) 
\end{equation}
Onde
\begin{equation}
\text{sim}(I_1,I_2,I_f|w) = \left\{
\begin{array}{ccc}
    0   & \text{if} &  \frac{\sigma_{I_1I_f}}{\sigma_{I_1I_f}\sigma_{I_2I_f}} > 0  \\
    \frac{\sigma_{I_1I_f}}{\sigma_{I_1I_f}\sigma_{I_2I_f}}  & \text{if} &  0\le \frac{\sigma_{I_1I_f}}{\sigma_{I_1I_f}\sigma_{I_2I_f}} \le 1  \\
        1   & \text{if} &  \frac{\sigma_{I_1I_f}}{\sigma_{I_1I_f}\sigma_{I_2I_f}} > 1  \\
\end{array}
\right.,
\end{equation}
OBS: Ler o Método do jacobiano para descobrir a densidade!!!! 
\bibliographystyle{IEEEtran}
\bibliography{bibliografia}

\end{document}